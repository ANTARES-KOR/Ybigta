{"cells":[{"cell_type":"markdown","metadata":{"id":"rzEaY-lN1yWH"},"source":["# 텍스트 전처리 (Text Preprocessing)"]},{"cell_type":"markdown","metadata":{"id":"noOGuHR-1yWS"},"source":["credit: 18기 DA 김채형 <br>\n","19기 DA 박수민"]},{"cell_type":"markdown","source":["## 0. NLTK 설치"],"metadata":{"id":"RSPff_RMA6Bq"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tl_0q5YiBAB1","executionInfo":{"status":"ok","timestamp":1644136504157,"user_tz":-540,"elapsed":279,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"bf21c059-c75b-46a9-adcf-f8142729fbd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"lKro5KXk1yWV"},"source":["## 1. 문장 토큰화 (Sentence Tokenization) "]},{"cell_type":"markdown","metadata":{"id":"6bDQPIme1yWV"},"source":["- 문장 토큰화는 토큰의 단위를 문장으로 하여, 코퍼스 내 텍스트를 문장 단위로 구분하는 작업을 의미합니다. \n","- 영어의 경우 NLTK의 `sent_tokenize`를 사용하여 영어 문장 토큰화를 수행할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05YgkurB1yWY","executionInfo":{"status":"ok","timestamp":1644133887513,"user_tz":-540,"elapsed":262,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"0829fd3c-3907-4481-e262-dd52273262ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"]}],"source":["from nltk.tokenize import sent_tokenize\n","text = 'His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.'\n","print(sent_tokenize(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tKP5Ts71yWb","executionInfo":{"status":"ok","timestamp":1644133901217,"user_tz":-540,"elapsed":10,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"e86a641f-fcb8-4e2b-a5f7-ea1b1d93c3a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"]}],"source":["# 문장 중간에 .이 있는 경우\n","from nltk.tokenize import sent_tokenize\n","text = 'I am actively looking for Ph.D. students. and you are a Ph.D student.'\n","print(sent_tokenize(text))"]},{"cell_type":"markdown","metadata":{"id":"f9pZPAXx1yWe"},"source":["## 2. 단어 토큰화 (Word Tokenization)"]},{"cell_type":"markdown","metadata":{"id":"9TY1OCHp1yWh"},"source":["- 단어 토큰화는 토큰의 단위를 단어로 하여, 코퍼스 내 텍스트를 단어 단위로 구분하는 작업을 의미합니다. \n","- 영어의 경우 텍스트를 단어 단위로 구분할 때 보통 띄어쓰기 즉 공백(whitespace)을 기준으로 합니다.\n","- ex) text : Time is an illusion. Lunchtime double so! <br>\n","tokenized : \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\" <br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whlzBG571yWk","executionInfo":{"status":"ok","timestamp":1644134308940,"user_tz":-540,"elapsed":388,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"c732b234-15ac-4cb9-ed4e-1c3118fad5ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"]}],"source":["# 단어 토큰화 word_tokenize\n","# (Don't => Do 와 n't 로 구분/ Jone's => Jone 와 's로 구분 )\n","\n","from nltk.tokenize import word_tokenize\n","text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n","print(word_tokenize(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3TVd3EZq1yWm","executionInfo":{"status":"ok","timestamp":1644134370104,"user_tz":-540,"elapsed":276,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"ce970ac8-face-48b2-9980-f3415d8533d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"]}],"source":["# WordPunctTokenizer: 구두점(punctuation)을 별도의 토큰으로 구분\n","# (Don't => Don 와 ' 와 t 로 구분  / Jone's => Jone 와 ' 와 s 로 구분)\n","\n","from nltk.tokenize import WordPunctTokenizer\n","text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n","print(WordPunctTokenizer().tokenize(text))"]},{"cell_type":"markdown","metadata":{"id":"bC7rM_O71yWp"},"source":["### Penn Treebank Tokenization"]},{"cell_type":"markdown","metadata":{"id":"Yu5yReQt1yWq"},"source":["Penn Treebank Tokenization은 표준으로 쓰이고 있는 토큰화 방법 중 하나입니다. Penn Treebank Tokenization의 규칙은 다음과 같습니다.\n","\n","규칙 1. 하이픈 (-)으로 구성된 단어는 하나로 유지한다.  \n","규칙 2. 아포스트로피 (') 로 접어가 함께 하는 단어는 분리한다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGREY9vD1yWr","executionInfo":{"status":"ok","timestamp":1644134419500,"user_tz":-540,"elapsed":262,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"958ba359-9ffd-4fa0-8ae2-fbec90f607a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"]}],"source":["from nltk.tokenize import TreebankWordTokenizer\n","tokenizer = TreebankWordTokenizer()\n","text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n","print(tokenizer.tokenize(text))"]},{"cell_type":"markdown","metadata":{"id":"4qUZV_-N1yWt"},"source":["## 3. 형태소 분석"]},{"cell_type":"markdown","metadata":{"id":"wgwT-q3Z1yWu"},"source":["- 영어의 경우 단어 토큰화를 수행할 때 띄어쓰기를 단어 구분 기준으로 하는데, 이를 어절 토큰화라고 합니다. \n","<br>\n","- 그런데 한국어의 경우 단어 토큰화를 수행할 때 어절 토큰화를 사용하는 것은 부적절합니다. \n","- 이는 한국어가 교착어((조사, 어미 등을 붙여서 말을 만드는 언어))라는 점에 기인합니다. <br>\n","      \"그\" + 조사 -> \"그를\", \"그에게\", \"그가\" \n","      \"즐겁다\" + 어미 -> \"즐거운\", \"즐거워서\", \"즐겁게\" \n","      하지만 이는 다른 단어가 아님! 따라서 조사와 어미 등을 분리해야 함.\n","\n","- 대신, 한국어의 경우 단어 토큰화를 수행할 때 토큰의 단위를 형태소로 하는 **형태소 토큰화**를 사용합니다.\n","        형태소(morpheme): 뜻을 가진 가장 작은 말의 단위\n","        예) \"아버지가 방에 들어가신다\"\n","        -> ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다']\n","\n","- 한국어 텍스트 전처리 내용 -> \"한국어_텍스트_전처리.ipynb\" 파일 참고!"]},{"cell_type":"markdown","metadata":{"id":"gP_-6HJy1yWu"},"source":["## 4. 품사 태깅 (Part-Of-Speech Tagging ; POS Tagging)"]},{"cell_type":"markdown","metadata":{"id":"B4-2mHyH1yWv"},"source":["- 때때로 단어는 표기는 같지만 품사에 따라 단어의 의미가 달라지는 경우가 발생합니다.\n","- 예) \"fly\" -> 날다, 파리\n","- 따라서, 단어의 의미를 제대로 파악하기 위해서는 해당 단어의 품사 정보가 필요합니다. \n","- 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지 구분하는 것을 품사 태깅(Part-Of-Speech tagging ; POS Tagging)이라고 합니다.\n","\n","**Penn Treebank**\n","\n","|Number|Tag|Description|\n","|------|---|-----------|\n","|1.|CC |Coordinating conjunction|\n","|2.|CD |Cardinal number|\n","|3.|DT |Determiner|\n","|4.|EX |Existential there|\n","|5.|FW |Foreign word|\n","|6.|IN |Preposition or subordinating conjunction|\n","|7.|JJ |Adjective|\n","|8.|JJR|Adjective, comparative|\n","|9.|JJS|Adjective, superlative|\n","|10.|LS|List item marker|\n","|11.|MD|Modal|\n","|12.|NN|Noun, singular or mass|\n","|13.|NNS|Noun, plural|\n","|14.|NNP|Proper noun, singular|\n","|15.|NNPS|Proper noun, plural|\n","|16.|PDT|Predeterminer|\n","|17.|POS|Possessive ending|\n","|18.|PRP|Personal pronoun|\n","|19.|PRP\\\\$|Possessive pronoun|\n","|20.|RB |Adverb|\n","|21.|RBR|Adverb, comparative|\n","|22.|RBS|Adverb, superlative|\n","|23.|RP\t|Particle|\n","|24.|SYM|Symbol|\n","|25.|TO\t|to|\n","|26.|UH\t|Interjection|\n","|27.|VB\t|Verb, base form|\n","|28.|VBD|Verb, past tense|\n","|29.|VBG|Verb, gerund or present participle|\n","|30.|VBN|Verb, past participle|\n","|31.|VBP|Verb, non-3rd person singular present|\n","|32.|VBZ|Verb, 3rd person singular present|\n","|33.|WDT|Wh-determiner|\n","|34.|WP\t|Wh-pronoun|\n","|35.|WP$|Possessive wh-pronoun|\n","|36.|WRB|Wh-adverb|\n"]},{"cell_type":"markdown","metadata":{"id":"LCj6vAxw1yWw"},"source":["## 5. 어간 추출 (Stemming) & 원형 복원 (Lemmatization)"]},{"cell_type":"markdown","metadata":{"id":"JTXiKr6i1yWw"},"source":["- 단어의 형태 변화(lexical variations of term ; term variation) 에 따라 같은 단어라도 다른 단어인 것처럼 취급되는 문제를 해결하기 위해 사용되는 보편적인 방법으로 어간 추출(Stemming)과 원형 복원(Lemmatization)이 있습니다."]},{"cell_type":"markdown","metadata":{"id":"13WUA4VL1yWx"},"source":["### Stemming (어간 추출)"]},{"cell_type":"markdown","metadata":{"id":"vfGJl9Is1yWx"},"source":["- Stemming이란 어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해내는 것을 의미합니다. \n","        예) 'automate', 'automatic', 'automation' -> 'automat' \n","        각각 모두 'automat' 어간 + 'e', 'ic', 'ion'이라는 접사 \n","- 이러한 단어들에 대하여 접사를 제거하고 동일한 어간인 'automat'으로 매핑되도록 하는 작업이 stemming입니다.\n","\n","- 대표적인 Stemming Algorithm으로 Martin Porter가 고안한 Porter Stemming Algorithm = Porter Stemmer 가 있습니다. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hjtGBbml1yWx","executionInfo":{"status":"ok","timestamp":1644135762817,"user_tz":-540,"elapsed":271,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"63577522-c648-4fa4-800d-a997c5361854"},"outputs":[{"output_type":"stream","name":"stdout","text":["original text: This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\n","tokenized words: ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n","stemmed words: ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"]}],"source":["from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","\n","text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n","\n","# word tokenization\n","words = word_tokenize(text)\n","\n","# stemming\n","s = PorterStemmer()\n","result = [s.stem(w) for w in words]\n","\n","# 결과 출력\n","print('original text:', text)\n","print('tokenized words:',words)\n","print('stemmed words:',result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SE7MIqAP1yWy","executionInfo":{"status":"ok","timestamp":1644135962958,"user_tz":-540,"elapsed":7,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"5e54f8b8-bd45-4be0-9ee4-d19d657e81b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["original words: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n","porter stemmer: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n","lancaster stemmer: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"]}],"source":["## 두가지 종류의 stemmer 비교\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","\n","words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n","\n","# stemming\n","s = PorterStemmer() # 포터 스태머\n","l = LancasterStemmer() # 랭커스터 스태머\n","ss = [s.stem(w) for w in words]\n","ll = [l.stem(w) for w in words]\n","\n","# 결과 출력\n","print('original words:', words)\n","print('porter stemmer:', ss)\n","print('lancaster stemmer:',ll)"]},{"cell_type":"markdown","metadata":{"id":"EMSZ1i7K1yWz"},"source":["### Lemmatization (원형 복원 ; 표제어 추출)"]},{"cell_type":"markdown","metadata":{"id":"yr9FUVFA1yWz"},"source":["- Lemmatization은 한 단어가 여러 형식으로 표현되어 있는 것을 단일 형식으로 묶어주는 기법입니다. \n","        ex) 'am', 'are', 'is' -> 'be'\n","- Lemmatization을 수행할 경우, 품사 정보가 남아있기 때문에 의미론적 관점에서 더 효과적입니다. \n","- 하지만, 여전히 품사정보를 가지고 있어 stemming만큼 DTM dimension reduction 측면에서 효과적이진 않습니다. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4put6LUh1yW0","executionInfo":{"status":"ok","timestamp":1644136121467,"user_tz":-540,"elapsed":2117,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"c60119d1-bcfe-43d2-cf8a-5e22f2eb8799"},"outputs":[{"output_type":"stream","name":"stdout","text":["tokenized words: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n","lemmatized words: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","\n","words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n","\n","# lemmatization\n","n = WordNetLemmatizer()\n","result = [n.lemmatize(w) for w in words]\n","\n","# 결과 출력\n","print('tokenized words:',words)\n","print('lemmatized words:',result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVH4USoK1yW0","executionInfo":{"status":"ok","timestamp":1644136149002,"user_tz":-540,"elapsed":7,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"47738588-1abe-4dc8-b41f-7e199e7f708c"},"outputs":[{"output_type":"stream","name":"stdout","text":["die\n","watch\n","have\n"]}],"source":["# 단어의 품사 정보를 알려주어 다시 출력 -> 더 정확한 lemmatization\n","print(n.lemmatize('dies', 'v'))\n","print(n.lemmatize('watched', 'v'))\n","print(n.lemmatize('has', 'v'))"]},{"cell_type":"markdown","metadata":{"id":"0TQ837er1yW1"},"source":["### Stemming vs. Lemmatization"]},{"cell_type":"markdown","metadata":{"id":"-yyXkEQd1yW2"},"source":["|비교|Stemming|Lemmatization|\n","|-|--------|-------------|\n","|의미|어간 추출|원형 복원|\n","|접근 방법|정보검색적|언어학적|\n","|DTM dimension reduction 관점|good|bad|\n","|의미론적 관점|bad (품사 X)|good (품사 O)|\n","\n","- 영어 텍스트의 경우에는 stemming과 lemmatization이 명확하게 구분되어 텍스트 전처리 과정에서 무엇을 사용할지를 결정해야 합니다. \n","- 반면 한글 텍스트의 경우에는 형태소 분석 과정에서 stemming과 lemmatization이 함께 이루어진다고 볼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"x6DbOqYf1yW2"},"source":["## 6. Stopwords Removal (불용어 제거)"]},{"cell_type":"markdown","metadata":{"id":"VposhOA_1yW3"},"source":["- 너무 자주 나타나는 단어들은 기능적인 역할을 하거나 문헌집단 전반에 걸쳐 나타나기 때문에 특정 문헌의 내용을 대표할 수 없습니다. \n","- 자연어 말뭉치 표현에 나타나는 단어들을 그 사용 빈도가 높은 순서대로 나열하였을 때, 왼쪽에 존재하는 고빈도 단어들을 **stopwords**라고 합니다.\n","- 영어: 정관사, 전치사 등/ 한글: 조사 등\n","<br>\n","- 불용어 제거는 단어 정제를 통해 보다 제대로 된 분석을 하기 위함이기도 하고, 차원을 축소하기 위함이기도 합니다."]},{"cell_type":"markdown","metadata":{"id":"bad6bdEG1yW3"},"source":["### NLTK에서 정의한 불용어 리스트 사용하기 \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Is30D8Kx1yW4"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I63xZ3Ga1yW4","executionInfo":{"status":"ok","timestamp":1644136511515,"user_tz":-540,"elapsed":288,"user":{"displayName":"Soo Min Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04363060217763346939"}},"outputId":"4f50be0a-79ce-40f8-d62f-0312f9994b41"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n","['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"]}],"source":["example = \"Family is not an important thing. It's everything.\"\n","\n","# 불용어 리스트 생성\n","stop_words = set(stopwords.words('english')) \n","\n","# 단어 토큰화 실시\n","word_tokens = word_tokenize(example)\n","\n","# 단어 토큰화 결과로부터 불용어 제거 실시\n","result = []\n","for w in word_tokens: \n","    if w not in stop_words: \n","        result.append(w) \n","\n","# 결과 출력\n","print(word_tokens) \n","print(result) "]},{"cell_type":"markdown","metadata":{"id":"0UL2_rF-1yW5"},"source":["NLTK에서 제공하는 불용어 리스트를 활용하여 불용어를 제거한 결과 'is', 'not', 'an'과 같은 단어들이 제거된 것을 볼 수 있습니다."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"1_Text_Preprocessing_using_NLTK.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}
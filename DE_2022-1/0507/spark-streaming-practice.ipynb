{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 928,
     "status": "ok",
     "timestamp": 1636701600186,
     "user": {
      "displayName": "유임성",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07728430802442409044"
     },
     "user_tz": -540
    },
    "id": "GNLNoYoV2Cuz",
    "outputId": "4bb8bb85-3b97-4d0a-b883-794fe52d0429"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"streaming\").setMaster(\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ccc4b4944f82:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=streaming>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 771,
     "status": "error",
     "timestamp": 1636701606963,
     "user": {
      "displayName": "유임성",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07728430802442409044"
     },
     "user_tz": -540
    },
    "id": "sSE9fOuI2Cu3",
    "outputId": "c20f663c-c5bb-4f0f-f178-e95981bb5935"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 불러오기\n",
    "#static = spark.read.json('file:///home/ubuntu/Spark-The-Definitive-Guide/data/activity-data/') 아래것도 같음. 다만 이게 더 짧을 뿐..\n",
    "static = spark.read.format('json').load('./activity-data/')\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISuEsiEa2Cu4"
   },
   "outputs": [],
   "source": [
    "# 데이터 스키마 형태\n",
    "print(dataSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgS6Bpqn2Cu5"
   },
   "outputs": [],
   "source": [
    "# 데이터 프레임\n",
    "static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wXbA7xx2Cu5"
   },
   "outputs": [],
   "source": [
    "# 기본적으로 정적 구조적 API의 모든 트랜스포메이션은 스트리밍 DataFrame에서도 사용 가능\n",
    "# 구조적 스트리밍에서 스키마 추론 기능을 사용하고 싶을 경우 명시적으로 설정해야함\n",
    "# maxFilesPerTrigger는 폴더 내의 전체 파일을 얼마나 빨리 읽을지 결정해주는 파라미터. 낮게 잡으면 트리거당 하나의 파일을 읽게 만들어 스트림의 흐름을 인위적으로 제한할 수 있음\n",
    "streaming = spark.readStream.schema(dataSchema).option('maxFilesPerTrigger', 1).json('file:///home/ubuntu/ybigtatask/activity-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIzENgIz2Cu5"
   },
   "outputs": [],
   "source": [
    "# 스트리밍 DataFrame은 지연 처리 방식으로 동작함\n",
    "# 스트림 처리를 시작하는 액션을 호출하기 전에 스트리밍 DataFrame에 대한 트랜스포메이션을 지정할 수 있음\n",
    "activityCounts = streaming.groupBy('gt').count()\n",
    "print(activityCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ib_KFzQe2Cu6"
   },
   "outputs": [],
   "source": [
    "# 스트림 쿼리를 시작하는 액션 \n",
    "# 쿼리 결과를 내보낼 목적지나 싱크를 지정해야함 -> 예제에서는 결과를 메모리에 저장하는 메모리 싱크 사용\n",
    "# 스트림 처리에 사용되는 쿼리의 이름을 activity_counts로 설정\n",
    "# memory VS console\n",
    "activityQuery = activityCounts.writeStream.queryName('activity_counts').format('memory').outputMode('complete').start()\n",
    "\n",
    "# activityQuery.awaitTermination()  주석 풀면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9pSY88N2Cu6"
   },
   "outputs": [],
   "source": [
    "# 스트림 처리 중에 집계 결과가 저장된 메모리 테이블을 조회할 수 있음\n",
    "# 시점마다 다른 결과가 반환됨\n",
    "from time import sleep\n",
    "\n",
    "for x in range(5):\n",
    "    spark.sql('SELECT * FROM activity_counts').show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ws796vT2Cu7"
   },
   "outputs": [],
   "source": [
    "# 예제 실행 코드\n",
    "# 백그라운드에서 스트리밍 연산 실행됨. 쿼리 실행 중 드라이버 프로세스 종료되는 상황 방지\n",
    "# 실행시 끝날때까지 기다려야하기 때문에 실행 ㄴㄴ\n",
    "# 운영시 반드시 필요한 코드. 없으면 스트림 처리 실행할 수 없음\n",
    "activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCf-GN6o2Cu7"
   },
   "outputs": [],
   "source": [
    "# 스트림 중지\n",
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vJ-LTnf2Cu8"
   },
   "outputs": [],
   "source": [
    "# 현재 실행중인 스트림 목록\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw8YSgpS2Cu8"
   },
   "source": [
    "##  스트림 트랜스포메이션\n",
    "- 모든 유형의 선택과 필터, 그리고 트랜스포메이션뿐만 아니라 DataFrame의 모든 함수와 개별 컬럼 처리도 지원함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccFJD9jp2Cu9"
   },
   "source": [
    "### 선택과 필터링\n",
    "- 구조적 스트리밍은 DataFrame의 모든 함수와 개별 컬럼을 처리하는 선택과 필터링 그리고 단순 트랜스포메이션을 지원함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIFRlLlK2Cu-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "simpleTransform = streaming.withColumn('stairs', expr(\"gt like '%stairs%'\")).where('stairs').where('gt is not null')\\\n",
    "    .select('gt', 'model', 'arrival_time', 'creation_time')\\\n",
    "    .writeStream\\\n",
    "    .queryName('simple_transform')\\\n",
    "    .format('memory')\\\n",
    "    .outputMode('append')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuuBFmsi2Cu_"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for x in range(5):\n",
    "    spark.sql('SELECT * FROM simple_transform').show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMrwm3MU2CvA"
   },
   "outputs": [],
   "source": [
    "simpleTransform.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5mwCgQZ2CvA"
   },
   "source": [
    "### 집계(aggregate) - Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "error",
     "timestamp": 1637406730399,
     "user": {
      "displayName": "주우진",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18417040899282978373"
     },
     "user_tz": -540
    },
    "id": "Ecj4j-yy2CvB",
    "outputId": "1b1fa19e-462b-434b-ee22-7e1dc8941b6d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c24a9e3c37e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeviceModelStats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg(Arrival_time)'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg(Creation_Time)'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg(Index)'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device_counts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'streaming' is not defined"
     ]
    }
   ],
   "source": [
    "deviceModelStats = streaming.cube('gt', 'model').avg()\\\n",
    "    .drop('avg(Arrival_time)')\\\n",
    "    .drop('avg(Creation_Time)')\\\n",
    "    .drop('avg(Index)')\\\n",
    "    .writeStream.queryName('device_counts').format('memory')\\\n",
    "    .outputMode('complete')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNV1LRqW2CvC"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for x in range(5):\n",
    "    spark.sql('SELECT * FROM device_counts').show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ueRx5fB2CvD"
   },
   "outputs": [],
   "source": [
    "deviceModelStats.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WfHtPOm2CvD"
   },
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93OhlbxZ2CvE"
   },
   "outputs": [],
   "source": [
    "# cube?\n",
    "historicalAgg = static.groupBy('gt', 'model').avg()\n",
    "deviceModelStats = streaming.drop('Arrival_Time', 'Creation_Time', 'Index')\\\n",
    "    .cube('gt','model').avg()\\\n",
    "    .join(historicalAgg, ['gt', 'model'])\\\n",
    "    .writeStream.queryName('device_counts').format('memory')\\\n",
    "    .outputMode('complete')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vt6fVbU2CvE"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for x in range(5):\n",
    "    spark.sql('SELECT * FROM device_counts').show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EnIjCql2CvF"
   },
   "outputs": [],
   "source": [
    "deviceModelStats.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKkmwTRW2CvF"
   },
   "source": [
    "# 스트림 입출력\n",
    "\n",
    "## 스트림 싱크 예시(카프카)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUw3Q14I2CvG"
   },
   "outputs": [],
   "source": [
    "# ex\n",
    "df1 = spark.readStream.format('kafka')\\\n",
    "    .option('kafaka.bootstrap.servers', 'host1:port1, host2:port2')\\\n",
    "    .option('subscribe', 'topic1')\\\n",
    "    .load()\n",
    "\n",
    "# 여러 개의 토픽 수신\n",
    "df1 = spark.readStream.format('kafka')\\\n",
    "    .option('kafaka.bootstrap.servers', 'host1:port1, host2:port2')\\\n",
    "    .option('subscribe', 'topic1, topic2')\\\n",
    "    .load()\n",
    "\n",
    "# 패턴에 맞는 토픽 수신\n",
    "df1 = spark.readStream.format('kafka')\\\n",
    "    .option('kafaka.bootstrap.servers', 'host1:port1, host2:port2')\\\n",
    "    .option('subscribe', 'topic.*')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME7Z-4KA2CvG"
   },
   "outputs": [],
   "source": [
    "df1.selectExpr('topic', 'CAST(key AS STRING)', 'CAST(value AS STRING)')\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'host1:port1,host2:port2')\\\n",
    "    .option('checkpointLocation', '/to/HDFS-compatible/dir')\\\n",
    "    .start()\n",
    "\n",
    "df1.selectExpr('topic', 'CAST(key AS STRING)', 'CAST(value AS STRING)')\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'host1:port1,host2:port2')\\\n",
    "    .option('checkpointLocation', '/to/HDFS-compatible/dir')\\\n",
    "    .option('topic', 'topic1')\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vciKwwtC2CvG"
   },
   "source": [
    "## 테스트용 소스와 싱크\n",
    "- 스트리밍 쿼리의 프로토타입을 만들거나 디버깅 시 유용한 몇 가지 테스트용 소스와 싱크를 제공함\n",
    "- 운영환경에서는 사용하면 안됨, 종단 간 내고장성을 지원하지 않기 때문에 개발 시에만 사용\n",
    "\n",
    "- 소켓 소스\n",
    "  - TCP 소켓을 통해 스트림 데이터를 전송할 수 있음\n",
    "  - 데이터를 읽기 위한 호스트와 포트를 지정해야함\n",
    "  - 디버깅엔 유용하지만 내고장성 보장하지 못함\n",
    "  - nc -lk 9999 명령어를 통해 NetCat 사용하기\n",
    "  - Unix계열은 다 가능. EC2 컴퓨터에 새롭게 접속해서 새로운 Shell에서 nc -lk 9999 실행하기\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFXqPnXl2CvH"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "\n",
    "\n",
    "# # \" \"로 구분된 item과 price를 받고 각각 item과 price라는 column으로 넣기\n",
    "# items = lines.withColumn('item', split(lines.value, \" \")[0])\\\n",
    "#               .withColumn('price', split(lines.value, \" \")[1].cast(DoubleType()))\n",
    "\n",
    "# # item별로 price 평균 구하기\n",
    "# getAvg = items.groupBy(\"item\").avg(\"price\")\n",
    "\n",
    "# word count, 평균 구하기 둘 중 하나 or 둘 다 실습 때 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAsTgaia2CvH"
   },
   "outputs": [],
   "source": [
    "# update VS complete ?\n",
    "\n",
    "# query = getAvg \\\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .queryName('socket_word') \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "#format을 console로 하면 테이블이 console에서 나타남.\n",
    "#그렇다면.. 아래 Shell에서 코드 돌리면 나올까? output으로 나타나게 하는 방법은..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QXMxSLm2CvH"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for x in range(5):\n",
    "    spark.sql('SELECT * FROM socket_word').show()\n",
    "    sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Y4_Uasx2CvI"
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRlpZMgM2CvJ"
   },
   "source": [
    "## 트리거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cruixH-2CvK"
   },
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option('maxFilesPerTrigger', 1).json('file:///home/ubuntu/ybigta_session_homework/Spark-The-Definitive-Guide/data/activity-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcLbJCu52CvL"
   },
   "outputs": [],
   "source": [
    "activityCounts=streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62yJIFIk2CvM"
   },
   "outputs": [],
   "source": [
    "# 처리 시간 기반 트리거\n",
    "a = activityCounts.writeStream.trigger(processingTime='5 seconds').queryName('simple_transform').format('console').outputMode('append').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWpk3QnW2CvN"
   },
   "outputs": [],
   "source": [
    "a.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63Dxarjo2CvN"
   },
   "outputs": [],
   "source": [
    "b = activityCounts.writeStream.trigger(once=True).format('console').outputMode('append').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAXpGsDm2CvN"
   },
   "outputs": [],
   "source": [
    "b.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2WfHtPOm2CvD"
   ],
   "name": "spark-streaming-practice.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
